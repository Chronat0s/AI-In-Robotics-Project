<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI In Robotics Project</title>
    <link rel="shortcut icon" type="image/jpg" href="./images/small-profile.jpeg">
    <style>
        body {
            margin: 0;
            font-family: Arial, sans-serif;
        }
        
        .section {
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            padding: 60px;
        }
        
        .section img {
            max-width: 100%;
            height: auto;
            margin: 20px;
        }
        
        .text-section {
            text-align: center;
            padding: 20px;
            background-size: cover;
        }
        
        .text-section img {
            height: 150px;
            border-radius: 50%;
            border: 10px solid #FEDE00;
            margin-bottom: 20px;
        }
        
        .text-section h1 {
            font-size: 2.5rem;
            color: black;
            margin: 10px;
        }
        
        .text-section p {
            font-size: 1.25rem;
            color: black;
        }
        
        h1 {
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 4rem;
            text-align: center;
            padding: 20px;
            margin: 0;
        }
        
        .content-section {
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            padding: 20px;
        }
        
        .content-section img {
            max-width: 100%;
            height: auto;
        }
        
        .content-section div {
            max-width: 1400px;
            margin: 20px;
        }
        
        .content-section p {
            line-height: 1.6;
            font-size: 1rem;
            padding: 20px;
        }
        
        .content-section ul {
            line-height: 1.6;
            font-size: 1rem;
            padding: 20px;
        }
        
        .video-wrapper {
            position: relative;
            width: 100%;
            margin: 0 auto;
            padding-bottom: 56.25%;
            /* 16:9 aspect ratio */
            height: 0;
            overflow: hidden;
        }
        
        .video-wrapper iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        
        @media (min-width: 768px) {
            .text-section h1 {
                font-size: 3rem;
            }
            .content-section p {
                font-size: 1.25rem;
            }
        }
        
        @media (min-width: 1024px) {
            .text-section h1 {
                font-size: 4rem;
            }
            .content-section p {
                font-size: 1.5rem;
            }
        }
    </style>
</head>

<body>
    <!-- Second section -->
    <div class="section">
        <div class="text-section" style="background-image: url('images/background.jpg'); height: 600px;">
            <img src="./images/small-profile.jpeg" alt="Group member profile picture">
            <h1>AI in Robotics Group 8</h1>
            <p><em>Damian Sue 13900156<br />
                    Morne Kruger 13926757<br />Yara Fakoua 13914510<br />Iraklis
                    Roussos
                    13699956</em></p>
        </div>
        <img src="./images/kuka.jpg" style="height: 600px;" alt="KUKA robot arm">
    </div>

    <h1>Problem Scenario</h1>
    <div class="content-section">
        <img src="./images/problem_scenario.jpg" alt="Problem scenario image">
        <div>
            <p>Our problem scenario is defined by three characterisations: the challenge, the objective, and the approach. The challenge is that robots currently operate in dynamic environments and must avoid potential collisions that could cause injury
                or damage equipment. Our objective, therefore, is to develop a collision avoidance system that uses reinforcement learning to train a robot arm to navigate safely in these dynamic environments. The approach involves utilising an RGB and
                depth camera to provide real-time input of thrown tennis balls, enabling the robot arm to avoid these obstacles.</p>
        </div>
    </div>

    <h1>Problem Solution</h1>
    <div class="content-section">
        <img src="./images/solution_flowchart.jpg" alt="Solution flowchart image">
        <div>
            <p>The flow diagram illustrates the solution to our problem scenario, where an RGB and depth camera provide real-time visual input of tennis balls being thrown at a robotic arm. Using the RGB camera, we employed YOLO to capture various colour
                variations and utilised these features for object detection. We then trained the model to recognise incoming obstacles and predict their bounding boxes. The depth camera served as input for the deep Q-learning model, which, using the predicted
                bounding boxes, enabled the robot arm to learn the optimal positions to avoid the incoming tennis balls.</p>
        </div>
    </div>

    <h1>Environment</h1>
    <div class="content-section">
        <img src="./images/Environment.png" alt="Environment image">
        <div>
            <p>The environment for our mission consists of a 7-degree-of-freedom KUKA LBR iiwa robotic arm mounted on a table within a Pybullet simulation. Seven-times-upscaled tennis balls move towards the robot's workspace from random positions. A synthetic
                RGB-D and depth camera is positioned to view the workspace and the incoming tennis balls, allowing the arm to detect and dodge these objects. Robot control was achieved using the pybullet-robot-envs repository from <a href="https://github.com/hsp-iit/pybullet-robot-envs.git"
                    target="_blank">GitHub</a>.</p>
        </div>
    </div>

    <h1>YOLOv8</h1>
    <div class="content-section">
        <img src="./images/yolo_graph1.jpg" alt="YOLOv8 graph 1">
        <img src="./images/yolo_graph2.jpg" alt="YOLOv8 graph 2">
        <img src="./images/yolo_graph3.jpg" alt="YOLOv8 graph 3">
        <div>
            <p>YOLO version 8 or You Only Look Once is a computer vision model developed by Ultralytics for the purpose of real-time image and video processing. It offers a short response time for detection, classification, and segmentation problems, which
                made it a good candidate for use in our project for tracking projectiles in real-time.
                <br /><br /> Utilising the Open Images v7 Dataset, which contains approximately 16 million images across 600 classes <a href="https://storage.googleapis.com/openimages/web/visualizer/index.html?type=detection&set=train&c=%2Fm%2F05ctyq"
                    target="_blank">see here</a>, we downloaded around 400 annotated images of tennis balls and organised them into training, validation, and test sets using a script.
                <br /><br /> Initially, we began with a detection model pre-trained on the entire Open Images v7 Dataset. During training, the loss for both boxes and classes consistently trended downwards, though the validation loss was not perfect.
                Our primary focus was on accurately detecting tennis balls. A confusion matrix would help assess the model's performance in this specific task.
                <br /><br /> The model reliably predicts tennis balls correctly, and the other classes, which are not of interest, are mostly dismissed as background, not affecting the model's implementation. This accuracy was visually confirmed on the
                test dataset and within the Pybullet environment.
            </p>
        </div>
    </div>

    <h1>DQN</h1>
    <div class="content-section">
        <img src="./images/DQN_graph1.jpg" style="max-width: calc(100/3);" alt="DQN graph">
        <img src="./images/DQN Results 1.png" style="max-width: calc(100/3);" alt="DQN Results 1">
        <img src="./images/DQN Results 2.png" style="max-width: calc(100/3);" alt="DQN Results 2">
        <div>
            <p>In our project, we utilise a Deep Q-Network (DQN) to enable the robotic arm to make informed movement decisions. The DQN model is essential for the robot's decision-making process, allowing it to learn and improve over time through interaction
                with the environment. The inputs to the DQN include the end effector's position and the distance to the nearest tennis ball, which help the robot understand its current state and the proximity of obstacles. The DQN architecture comprises
                two hidden layers with 256 nodes each, enabling the model to process these inputs effectively and learn the optimal actions to avoid collisions. The output of the DQN is an end effector movement command, which is then used alongside inverse
                kinematics to calculate the necessary joint angles for the robot to avoid obstacles. Essentially, the DQN model functions as the brain of our collision avoidance system, guiding the robot to navigate its environment safely in real-time.
                <br /><br /> The graphs shown illustrate the results of various training runs, where the x-axis represents the episodes and the y-axis represents the reward obtained. On the left, we see the initial run, and on the right is our most recent
                episode versus reward graph. In the initial version, the DQN learned that placing the end effector straight down on the table maximized the reward, which was effective for avoiding projectiles but not practical.
                <br /><br /> In subsequent versions, we refined the observations and introduced penalties and rewards for specific actions. This led to the DQN learning more balanced strategies for avoiding projectiles while maintaining an optimal position.
                The improvements can be seen in the increased number of positive reward episodes, indicating a more effective collision avoidance strategy.
            </p>
        </div>
    </div>

    <!-- Footer -->
    <h1>Demo Video</h1>
    <div class="section">
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/rsxKn5gjGoo" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/XS6jVz29KQw" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
    </div>
    <div class="content-section" style="display: flex; justify-content: center; align-items: center; flex-direction: column; max-width: 1400px; padding: 20px; margin: 0 auto;">
        <h1>Development:</h1>
        <h2>Environment: </h2>
        <p>Some compromises were made to simplify the environment when compared with a theoretical real-world counterpart: </p>
        <ol style="font-size: 1.5rem;">
            <li>
                Only one type of projectile (tennis balls) would be used for a simpler computer vision task.
            </li>
            <li>
                The tennis balls would be upscaled to increase visibility at lower resolutions to the YOLO model. This allowed the simulation to run faster instead of rendering a high resolution RGBD image every step.
            </li>
            <li>
                To assist with a meaningful reward scheme, projectiles would not be targeting areas on the robot where it cannot move and therefore avoid damage (the robot’s base and first link). </li>
        </ol>
        <p>The KUKA and table models were taken from the PyBullet library.
            <br /> <br /> Obstacle models were taken from pybullet-object-models on
            <a href="https://github.com/eleramp/pybullet-object-models/tree/master" target="_blank">Github</a>.<br /><br /> Control of the KUKA arm was achieved using code from pybullet-robot-envs on <a href="https://github.com/hsp-iit/pybullet-robot-envs"
                target="_blank">Github</a>.
        </p>
        <h2>YOLO:</h2>
        <p>After selecting YOLO for the project, we built our dataset of tennis ball images and found a model pre-trained on a larger dataset which we could fine-tune. <br /><br /> Annotated images of tennis balls were taken from the Open Images Dataset
            v7. An example of these can be found at this link: <a href="https://storage.googleapis.com/openimages/web/visualizer/index.html?type=detection&set=train&c=%2Fm%2F05ctyq " target="_blank">https://storage.googleapis.com/openimages/web/visualizer/index.html?type=detection&set=train&c=%2Fm%2F05ctyq
            </a> <br /><br /> The FiftyOne library was used to download and format the desired annotated images into training, test, and validation sets. A .yaml file was also generated for YOLO to read the dataset. A guide for this can be found here:
            <a href="https://docs.voxel51.com/user_guide/export_datasets.html#yolov5dataset " target="_blank">https://docs.voxel51.com/user_guide/export_datasets.html#yolov5dataset</a>
            <br /><br /> Pretrained models on the entire Open Images V7 dataset can be found here: <a href="https://docs.ultralytics.com/datasets/detect/open-images-v7/">https://docs.ultralytics.com/datasets/detect/open-images-v7/</a>. The basic YOLOv8n
            was used.
        </p>
        <h2>DQN:</h2>
        <p>Several rounds of training using a Pybullet simulation were performed to iteratively improve upon and find suitable hyperparameters as well as a reward scheme for the robot. During development, there were three major changes to how rewards were
            given to the DQN. </p>
        <ul style="font-size: 1.5rem;">
            <li>
                Attempt 1:
                <ul style="font-size: 1.5rem;">
                    <li>
                        Action: penalties were given for hitting a tennis ball.
                    </li>
                    <li>
                        Result: the robot would lay flat on the table and sway from side to side to avoid projectiles, abusing the fact that the least number of projectiles came through this region. This is not ideal in a real-world situation, as colliding with the table would
                        still cause damage.
                    </li>
                </ul>
            </li>
            <li>
                Attempt 2:
                <ul style="font-size: 1.5rem;">
                    <li>
                        Action: to combat the robot’s tendency to lay flat on the table to avoid the majority of obstacles and therefore maximise its reward per episode, a penalty was added for collision with the table. Rewards are also now given for maintaining distance from
                        an obstacle using the updated YOLO input observations.
                    </li>
                    <li>
                        Result: the robot showed a new tendency to adopt a position standing straight up. While this helped it accrue rewards for ‘dodging’ projectiles coming down on the sides, its manipulability in this pose was severely reduced. This resulted in a sure hit
                        for all projectiles coming down the centre.
                    </li>
                </ul>
            </li>
            <li>
                Attempt 3: Current
                <ul style="font-size: 1.5rem;">
                    <li>
                        Action: to increase the general manipulability of the robot to better respond to incoming projectiles, penalties were given for raising the end effector more than 1m above the table.
                    </li>
                    <li>
                        Result: the robot maintains a more manipulable pose and is better able to respond to projectiles.
                    </li>
                </ul>
            </li>
        </ul>
        <h2>Theoretical Improvements: </h2>
        <p>DQN is not an entirely appropriate solution for controlling the robot but is the limit of our current knowledge. Its main limitation is the requirement for a discrete action space. In the case of this project, this meant that commands for the
            end effector to move could only be given at predetermined speeds. Should there be a need to respond to different projectiles moving at different speeds, this would not be an adequate control method. A cursory search online shows that Deep
            Deterministic Policy Gradient (DDPG) uses a similar Q-function structure as DQN but allows for the use of a continuous action space. <br /><br /> Increasing the complexity of the observation space to track more than one projectile could also
            improve its ability to avoid more tennis balls at once. Additionally, changing the observations from the end effector’s distance to the nearest projectile to the distance from each joint to the projectile could help the robot sense when it
            is in danger faster. This would add much more complexity to the problem, however.
        </p>
    </div>
</body>

</html>