<!DOCTYPE html>
<html lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI In Robotics Project</title>
    <link rel="shortcut icon" type="image/jpg" href="./images/small-profile.jpeg">
    <style>
        body {
            margin: 0;
            font-family: Arial, sans-serif;
        }
        
        .section {
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            padding: 60px;
        }
        
        .section img {
            max-width: 100%;
            height: auto;
            margin: 20px;
        }
        
        .text-section {
            text-align: center;
            padding: 20px;
            background-size: cover;
        }
        
        .text-section img {
            height: 150px;
            border-radius: 50%;
            border: 10px solid #FEDE00;
            margin-bottom: 20px;
        }
        
        .text-section h1 {
            font-size: 2.5rem;
            color: black;
            margin: 10px;
        }
        
        .text-section p {
            font-size: 1.25rem;
            color: black;
        }
        
        h1 {
            display: flex;
            justify-content: center;
            align-items: center;
            font-size: 4rem;
            text-align: center;
            padding: 20px;
            margin: 0;
        }
        
        .content-section {
            display: flex;
            justify-content: center;
            align-items: center;
            flex-wrap: wrap;
            padding: 20px;
        }
        
        .content-section img {
            max-width: 100%;
            height: auto;
            margin: 20px;
        }
        
        .content-section div {
            max-width: 1400px;
            margin: 20px;
        }
        
        .content-section p {
            line-height: 1.6;
            font-size: 1rem;
            padding: 20px;
        }
        
        .video-wrapper {
            position: relative;
            width: 100%;
            margin: 0 auto;
            padding-bottom: 56.25%;
            /* 16:9 aspect ratio */
            height: 0;
            overflow: hidden;
        }
        
        .video-wrapper iframe {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
        }
        
        @media (min-width: 768px) {
            .text-section h1 {
                font-size: 3rem;
            }
            .content-section p {
                font-size: 1.25rem;
            }
        }
        
        @media (min-width: 1024px) {
            .text-section h1 {
                font-size: 4rem;
            }
            .content-section p {
                font-size: 1.5rem;
            }
        }
    </style>
</head>

<body>
    <!-- Second section -->
    <div class="section">
        <div class="text-section" style="background-image: url('images/background.jpg'); height: 600px;">
            <img src="./images/small-profile.jpeg" alt="Group member profile picture">
            <h1>AI in Robotics Group 8</h1>
            <p><em>Damian Sue 13900156<br />
          Morne Kruger 13926757<br />Yara Fakoua 13914510<br />Iraklis Roussos
          13699956</em></p>
        </div>
        <img src="./images/kuka.jpg" style="height: 600px;" alt="KUKA robot arm">
    </div>

    <h1>Problem Scenario</h1>
    <div class="content-section">
        <img src="./images/problem_scenario.jpg" alt="Problem scenario image">
        <div>
            <p>Our problem scenario is defined by three characterisations: the challenge, the objective, and the approach. The challenge is that robots currently operate in dynamic environments and must avoid potential collisions that could cause injury
                or damage equipment. Our objective, therefore, is to develop a collision avoidance system that uses reinforcement learning to train a robot arm to navigate safely in these dynamic environments. The approach involves utilising an RGB and
                depth camera to provide real-time input of thrown tennis balls, enabling the robot arm to avoid these obstacles.</p>
        </div>
    </div>

    <h1>Problem Solution</h1>
    <div class="content-section">
        <img src="./images/solution_flowchart.jpg" alt="Solution flowchart image">
        <div>
            <p>The flow diagram illustrates the solution to our problem scenario, where an RGB and depth camera provide real-time visual input of tennis balls being thrown at a robotic arm. Using the RGB camera, we employed YOLO to capture various colour
                variations and utilised these features for object detection. We then trained the model to recognise incoming obstacles and predict their bounding boxes. The depth camera served as input for the deep Q-learning model, which, using the predicted
                bounding boxes, enabled the robot arm to learn the optimal positions to avoid the incoming tennis balls.</p>
        </div>
    </div>

    <h1>Environment</h1>
    <div class="content-section">
        <img src="./images/Environment.png" alt="Environment image">
        <div>
            <p>The environment for our mission consists of a 7-degree-of-freedom KUKA LBR iiwa robotic arm mounted on a table within a Pybullet simulation. Seven-times-upscaled tennis balls move towards the robot's workspace from random positions. A synthetic
                RGB-D and depth camera is positioned to view the workspace and the incoming tennis balls, allowing the arm to detect and dodge these objects. Robot control was achieved using the pybullet-robot-envs repository from <a href="https://github.com/hsp-iit/pybullet-robot-envs.git"
                    target="_blank">GitHub</a>.</p>
        </div>
    </div>

    <h1>YOLOv8</h1>
    <div class="content-section">
        <img src="./images/yolo_graph1.jpg" alt="YOLOv8 graph 1">
        <img src="./images/yolo_graph2.jpg" alt="YOLOv8 graph 2">
        <img src="./images/yolo_graph3.jpg" alt="YOLOv8 graph 3">
        <div>
            <p>YOLO version 8 or You Only Look Once is a computer vision model developed by Ultralytics for the purpose of real-time image and video processing. It offers a short response time for detection, classification, and segmentation problems, which
                made it a good candidate for use in our project for tracking projectiles in real-time.
                <br /><br /> Utilising the Open Images v7 Dataset, which contains approximately 16 million images across 600 classes <a href="https://storage.googleapis.com/openimages/web/visualizer/index.html?type=detection&set=train&c=%2Fm%2F05ctyq"
                    target="_blank">see here</a>, we downloaded around 400 annotated images of tennis balls and organised them into training, validation, and test sets using a script.
                <br /><br /> Initially, we began with a detection model pre-trained on the entire Open Images v7 Dataset. During training, the loss for both boxes and classes consistently trended downwards, though the validation loss was not perfect.
                Our primary focus was on accurately detecting tennis balls. A confusion matrix would help assess the model's performance in this specific task.
                <br /><br /> The model reliably predicts tennis balls correctly, and the other classes, which are not of interest, are mostly dismissed as background, not affecting the model's implementation. This accuracy was visually confirmed on the
                test dataset and within the Pybullet environment.
            </p>
        </div>
    </div>

    <h1>DQN</h1>
    <div class="content-section">
        <img src="./images/DQN_graph1.jpg" alt="DQN graph">
        <div>
            <p>In our project, we utilise a Deep Q-Network (DQN) to enable the robotic arm to make informed movement decisions. The DQN model is essential for the robot's decision-making process, allowing it to learn and improve over time through interaction with the environment. The inputs to the DQN include the end effector's position and the distance to the nearest tennis ball, which help the robot understand its current state and the proximity of obstacles. The DQN architecture comprises two hidden layers with 256 nodes each, enabling the model to process these inputs effectively and learn the optimal actions to avoid collisions. The output of the DQN is an end effector movement command, which is then used alongside inverse kinematics to calculate the necessary joint angles for the robot to avoid obstacles. Essentially, the DQN model functions as the brain of our collision avoidance system, guiding the robot to navigate its environment safely in real-time.
               <br /><br /> The graphs shown illustrate the results of various training runs, where the x-axis represents the episodes and the y-axis represents the reward obtained. On the left, we see the initial run, and on the right is our most recent episode versus reward graph. In the initial version, the DQN learned that placing the end effector straight down on the table maximized the reward, which was effective for avoiding projectiles but not practical.
               <br /><br /> In subsequent versions, we refined the observations and introduced penalties and rewards for specific actions. This led to the DQN learning more balanced strategies for avoiding projectiles while maintaining an optimal position. The improvements can be seen in the increased number of positive reward episodes, indicating a more effective collision avoidance strategy.
            </p>
        </div>
    </div>

    <!-- Footer -->
    <h1>Demo Video</h1>
    <div class="section">
        <div class="video-wrapper">
            <iframe src="https://www.youtube.com/embed/IYnsfV5N2n8" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        </div>
    </div>
</body>

</html>
